# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jmLFe_vakFXQv4d3fPhM4He9AXxUTpAZ
"""

import numpy as np
import pandas as pd
import pickle # For saving the trained model
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix

#DATA PRE-PROCESSING

train_data = pd.read_csv("./models/dataset/mnist_train.csv")
test_data = pd.read_csv("./models/dataset/mnist_test.csv")
# Training data
x_train = train_data.iloc[:, 1:].astype(float)
y_train = train_data.iloc[:, 0]
x_train = x_train/255.0
# Testing data
x_test = test_data.iloc[:, 1:].astype(float)
y_test = test_data.iloc[:, 0]
x_test = x_test/255.0
# Printing data dimensions
print('x_train shape =', x_train.shape)
print('y_train shape =', y_train.shape)
print('x_test shape =', x_test.shape)
print('y_test shape =', y_test.shape)

class_names = [0,1,2,3,4,5,6,7,8,9]

def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):
	if axes is None:
		_, axes = plt.subplots(1, 1, figsize=(6, 5))

	axes.set_title(title)
	axes.set_xlabel("Training examples")
	axes.set_ylabel("Score")

	train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
																			train_sizes=train_sizes,
																			return_times=True)
	train_scores_mean = np.mean(train_scores, axis=1)
	train_scores_std = np.std(train_scores, axis=1)
	test_scores_mean = np.mean(test_scores, axis=1)
	test_scores_std = np.std(test_scores, axis=1)
	fit_times_mean = np.mean(fit_times, axis=1)
	fit_times_std = np.std(fit_times, axis=1)
	axes.grid()
	axes.fill_between(train_sizes, train_scores_mean-train_scores_std, train_scores_mean+train_scores_std, alpha=0.1, color="r")
	axes.fill_between(train_sizes, test_scores_mean-test_scores_std, test_scores_mean+test_scores_std, alpha=0.1, color="g")
	axes.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
	axes.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
	axes.legend(loc="best")
	plt.savefig("models/" + title + ".png")

def hyperparameter_tuning(classifier, params_grid):
	model_gs = GridSearchCV(estimator = classifier, param_grid = params_grid, scoring = 'accuracy', cv = 3, verbose = 3)
	model_gs.fit(x_train, y_train)
	best_score = model_gs.best_score_
	best_params = model_gs.best_params_
	return best_score, best_params

classifier = SVC(kernel = 'rbf')
params = [{'C':[1, 10, 100], 'gamma':[0.01, 0.001, 0.0001]}]
best_score, best_params = hyperparameter_tuning(classifier, params)
classifier = SVC(C = best_params['C'], gamma = best_params['gamma'], kernel="rbf")
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
print("Accuracy for model with tuned hyperparams =", accuracy_score(y_true = y_test, y_pred = y_pred), "\n")
print(confusion_matrix(y_true = y_test, y_pred = y_pred))

# Learning curve code
plot_learning_curve(classifier, "SVM", x_train, y_train, axes=None, ylim=(0.6, 1.01),  cv=3)